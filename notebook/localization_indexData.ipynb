{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nas4_user/hojuncho/kyudan/AnomLLM/AnomLLM\n",
      "/home/nas4_user/hojuncho/kyudan/AnomLLM/AnomLLM/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nas4_user/hojuncho/.cache/pypoetry/virtualenvs/anomllm-yItPHLz9-py3.10/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/nas4_user/hojuncho/kyudan/AnomLLM/AnomLLM/src'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n",
    "%cd ..\n",
    "%cd src\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please edit the BASE_DIR on your situation.\n",
    "\n",
    "BASE_DIR = \"/home/nas4_user/hojuncho/kyudan/AnomLLM/AnomLLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_localization_prompt(anomal_data):\n",
    "        \n",
    "    LOCALIZATION_PROMPT = f\"\"\"Detect the ranges of anomalies in this time series based on the x-axis coordinate, where the x-axis coordinate refers to the x value in (x, y) data. The patterns of anomalies are as follows.\n",
    "    {anomal_data}\\nList one by one, in JSON format. If there are no anomalies, answer with an empty list [].\n",
    "\n",
    "    Output template:\n",
    "    [{{\"start\": ..., \"end\": ...}}, {{\"start\": ..., \"end\": ...}}...]\n",
    "    \"\"\"\n",
    "    return LOCALIZATION_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gt_data(anomaly_locations)->list :\n",
    "    gt_data = []\n",
    "    for i in anomaly_locations:\n",
    "        anomal = i[0]\n",
    "        gt_data.append([[int(x) for x in sublist] for sublist in anomal])\n",
    "    return gt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_text = raw_results['gpt-4o-mini (0shot-text)']['request']['messages'][0]['content'][0]['text']\n",
    "#data_part = raw_text.split(\"\\n\\n\")[0]\n",
    "#data_part_list = data_part.\n",
    "import numpy as np\n",
    "from prompt import time_series_to_str\n",
    "\n",
    "def create_idx_datalist(arr:np.ndarray)->list:\n",
    "    \"\"\"\n",
    "    입력: array([[ 0.00000000e+00],\n",
    "       [ 1.87381312e-01],\n",
    "       [ 3.68124545e-01]]) 형태.\n",
    "    출력: [(0, '0.0'),\n",
    "            (1, '0.19'),\n",
    "            (2, '0.37')] 형태.\n",
    "    \"\"\"\n",
    "    idx_data_list  = []\n",
    "    for i, data in enumerate(time_series_to_str(arr).split(' ')):\n",
    "        idx_data_list.append((i,data))\n",
    "    return idx_data_list\n",
    "\n",
    "def create_idx_data(list_:list) -> str:\n",
    "    \"\"\"\n",
    "    입력: \n",
    "    [(0, '0.0'),\n",
    "    (1, '0.19'),\n",
    "    (2, '0.37')] \n",
    "\n",
    "    출력:\n",
    "    '(0,0.0), (1,0.19), (2,0.37)' 형태\n",
    "    \"\"\"\n",
    "    formatted_items = []\n",
    "    for item in list_:\n",
    "        formatted_item = f\"({item[0]},{item[1]})\"\n",
    "        formatted_items.append(formatted_item)\n",
    "    idx_data = ', '.join(formatted_items)\n",
    "    return idx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anomal_series(series_list, gt):\n",
    "    result_list = []\n",
    "    result = []\n",
    "    for diff_anom in gt:\n",
    "        # ground truth index는 1부터 1000이라 아래와 같이 indexing 해야 out of range error 안 뜸.\n",
    "        for i in range(diff_anom[0]-1,diff_anom[1]):\n",
    "            result_list.append(series_list[i])\n",
    "        result.append(result_list)\n",
    "        result_list = []\n",
    "    \n",
    "    return result # 0.19 0.37 0.54 이런 형태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anomal_data_prompt(anomal_list):\n",
    "    result = []\n",
    "    \n",
    "    for i, anomaly_group in enumerate(anomal_list, 1):\n",
    "        result.append(f\"anomaly data {i}:\")\n",
    "        \n",
    "        values = [item[1] for item in anomaly_group]\n",
    "        result.append(\" \".join(values))\n",
    "        \n",
    "        if i < len(anomal_list):\n",
    "            result.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import LIMIT_PROMPT\n",
    "\n",
    "\n",
    "def create_localization_prompt_idx(\n",
    "        idx,\n",
    "        series,\n",
    "        anomaly_locations,\n",
    "):\n",
    "    gt = create_gt_data(anomaly_locations)\n",
    "    idx_data = create_idx_data(create_idx_datalist(series[idx]))\n",
    "    #idx_list = create_idx_datalist(series[idx])\n",
    "    anomal_text = create_anomal_series(create_idx_datalist(series[idx]), gt[idx]) #j번째 anomal\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": idx_data \n",
    "                    + \"\\n\\n\" \n",
    "                    + LIMIT_PROMPT \n",
    "                    + make_localization_prompt(create_anomal_data_prompt(anomal_text))\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.4,\n",
    "        \"stop\": [\"''''\", \" – –\", \"<|endoftext|>\", \"<|eot_id|>\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_api import send_openai_request\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from loguru import logger\n",
    "from data.synthetic import SyntheticDataset\n",
    "\n",
    "def online_AD_with_retries(\n",
    "    model_name: str,\n",
    "    data_name: str,\n",
    "    #request_func: callable,\n",
    "    variant: str = \"standard\",\n",
    "    num_retries: int = 4,\n",
    "):\n",
    "         \n",
    "    # Initialize dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    # Configure logger\n",
    "    log_fn = f\"logs/synthetic/{data_name}/{model_name}/\" + variant + \".log\"\n",
    "    logger.add(log_fn, format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "    results_dir = f'{BASE_DIR}/results/synthetic/{data_name}/{model_name}/'\n",
    "    data_dir = f'{BASE_DIR}/data/synthetic/{data_name}/eval/'\n",
    "    train_dir = f'{BASE_DIR}/data/synthetic/{data_name}/train/'\n",
    "    jsonl_fn = os.path.join(results_dir, variant + '.jsonl')\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    eval_dataset = SyntheticDataset(data_dir)\n",
    "    eval_dataset.load()\n",
    "\n",
    "    train_dataset = SyntheticDataset(train_dir)\n",
    "    train_dataset.load()\n",
    "\n",
    "\n",
    "    # prepare series and anomaly locations(gt)\n",
    "    series = []\n",
    "    anomaly_locations = []\n",
    "    for idx in range(len(eval_dataset)): #(0부터 399)\n",
    "        series.append(eval_dataset[idx][1].numpy()) # eval dataset의 문제 series.\n",
    "        anomaly_locations.append(eval_dataset[idx][0].numpy()) # eval dataset의 gt가 될 것.\n",
    "    \n",
    "\n",
    "    # Load existing results if jsonl file exists\n",
    "    if os.path.exists(jsonl_fn):\n",
    "        with open(jsonl_fn, 'r') as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line.strip())\n",
    "                results[entry['custom_id']] = entry[\"response\"]\n",
    "\n",
    "    # Loop over image files\n",
    "    for i in range(0, len(eval_dataset)):\n",
    "        custom_id = f\"{data_name}_{model_name}_{variant}_{str(i).zfill(5)}\"\n",
    "        gt = create_gt_data(anomaly_locations)\n",
    "\n",
    "        # If there are no anomaly, response is forced to \"```json\\n[]\\n```\\n\"\n",
    "        # and others (custom_id, request) are same.\n",
    "        if len(gt[i]) == 0:\n",
    "            logger.info(f\"Setting empty list response for {custom_id} as it has no anomaly locations\")\n",
    "            request = create_localization_prompt_idx(i, series, anomaly_locations)\n",
    "            response = \"```json\\n[]\\n```\\n\"\n",
    "            # Write the result to jsonl\n",
    "            with open(jsonl_fn, 'a') as f:\n",
    "                json.dump({'custom_id': custom_id, 'request': request, 'response': response}, f)\n",
    "                f.write('\\n')\n",
    "            results[custom_id] = response\n",
    "            continue\n",
    "\n",
    "        # Skip already processed files\n",
    "        if custom_id in results:\n",
    "            continue\n",
    "        \n",
    "        # Perform anomaly detection with exponential backoff\n",
    "        for attempt in range(num_retries):\n",
    "            try:\n",
    "                request = create_localization_prompt_idx(i, series, anomaly_locations)\n",
    "                response = send_openai_request(request, model_name)\n",
    "                # Write the result to jsonl\n",
    "                with open(jsonl_fn, 'a') as f:\n",
    "                    json.dump({'custom_id': custom_id, 'request': request, 'response': response}, f)\n",
    "                    f.write('\\n')\n",
    "                # If successful, break the retry loop\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if \"503\" in str(e):  # Server not up yet, sleep until the server is up again\n",
    "                    while True:\n",
    "                        logger.debug(\"503 error, sleep 30 seconds\")\n",
    "                        time.sleep(30)\n",
    "                        try:\n",
    "                            response = send_openai_request(request, model_name)\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            if \"503\" not in str(e):\n",
    "                                break\n",
    "                else:\n",
    "                    logger.error(e)\n",
    "                    # If an exception occurs, wait and then retry\n",
    "                    wait_time = 2 ** (attempt + 3)\n",
    "                    logger.debug(f\"Attempt {attempt + 1} failed. Waiting for {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "        else:\n",
    "            logger.error(f\"Failed to process {custom_id} after {num_retries} attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_AD_with_retries('gemini-1.5-flash', 'trend', 'localization (indexData)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
