{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/home/kyudan/AnomLLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70782497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, jaccard_score\n",
    "\n",
    "def evaluate_multi_label(y_true, y_pred):\n",
    "\n",
    "    # Set label names\n",
    "    label_names = [\"Trend\", \"Seasonality\", \"Noise\", \"Anomalies\"]\n",
    "    \n",
    "    # Calculate individual evaluation metrics\n",
    "    metrics = {\n",
    "        # Exact match ratio (proportion of samples where all labels exactly match)\n",
    "        'exact_match_ratio': accuracy_score(y_true, y_pred),\n",
    "        \n",
    "        # Hamming loss (proportion of incorrectly predicted labels)\n",
    "        'hamming_loss': hamming_loss(y_true, y_pred),\n",
    "        \n",
    "        # Jaccard similarity (intersection/union)\n",
    "        'jaccard_score': jaccard_score(y_true, y_pred, average='samples'),\n",
    "        \n",
    "        # Per-label accuracy\n",
    "        'label_accuracy': {},\n",
    "        \n",
    "        # Detailed metrics (micro average: metrics calculated on the entire dataset)\n",
    "        'micro_precision': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        'micro_recall': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        'micro_f1': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "        \n",
    "        # Detailed metrics (macro average: average of metrics calculated for each label)\n",
    "        'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Calculate accuracy for each label\n",
    "    for i, label in enumerate(label_names):\n",
    "        label_true = y_true[:, i]\n",
    "        label_pred = y_pred[:, i]\n",
    "        metrics['label_accuracy'][label] = accuracy_score(label_true, label_pred)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def process_data(ground_truth_df, predictions_list):\n",
    "\n",
    "    # Extract values from prediction results\n",
    "    y_pred = []\n",
    "    \n",
    "    for pred_item in predictions_list:\n",
    "        # Extract prediction results from JSON response\n",
    "        response = pred_item[\"response\"]\n",
    "        # Extract JSON string (remove ```json and ``` and parse)\n",
    "        json_str = response.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        pred_dict = json.loads(json_str)\n",
    "        \n",
    "        # Convert to ordered list [Trend, Seasonality, Noise, Anomalies]\n",
    "        pred_values = [\n",
    "            pred_dict[\"Trend\"], \n",
    "            pred_dict[\"Seasonality\"], \n",
    "            pred_dict[\"Noise\"], \n",
    "            pred_dict[\"Anomalies\"]\n",
    "        ]\n",
    "        y_pred.append(pred_values)\n",
    "    \n",
    "    # Extract ground truth data\n",
    "    y_true = []\n",
    "    \n",
    "    # Get ground truth data for the number of prediction results\n",
    "    # Note: This assumes that ground truth and predictions are in the same order\n",
    "    for i in range(len(y_pred)):\n",
    "        if i < len(ground_truth_df):\n",
    "            row = ground_truth_df.iloc[i]\n",
    "            # [trend, seasonal, noise, has_anomaly]\n",
    "            true_values = [\n",
    "                row[\"trend\"], \n",
    "                row[\"seasonal\"], \n",
    "                row[\"noise\"], \n",
    "                row[\"has_anomaly\"]\n",
    "            ]\n",
    "            y_true.append(true_values)\n",
    "    \n",
    "    return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "def create_metrics_dataframe(metrics):\n",
    "\n",
    "    # 1. Convert overall metrics to DataFrame\n",
    "    overall_metrics = {\n",
    "        'Metric': [\n",
    "            'Exact Match Ratio',\n",
    "            'Hamming Loss',\n",
    "            'Jaccard Score',\n",
    "            'Micro-Precision',\n",
    "            'Micro-Recall',\n",
    "            'Micro-F1',\n",
    "            'Macro-Precision',\n",
    "            'Macro-Recall',\n",
    "            'Macro-F1'\n",
    "        ],\n",
    "        'Value': [\n",
    "            metrics['exact_match_ratio'],\n",
    "            metrics['hamming_loss'],\n",
    "            metrics['jaccard_score'],\n",
    "            metrics['micro_precision'],\n",
    "            metrics['micro_recall'],\n",
    "            metrics['micro_f1'],\n",
    "            metrics['macro_precision'],\n",
    "            metrics['macro_recall'],\n",
    "            metrics['macro_f1']\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    overall_df = pd.DataFrame(overall_metrics)\n",
    "    \n",
    "    # 2. Convert label-specific accuracy to DataFrame\n",
    "    label_accuracy = {\n",
    "        'Label': list(metrics['label_accuracy'].keys()),\n",
    "        'Accuracy': list(metrics['label_accuracy'].values())\n",
    "    }\n",
    "    \n",
    "    label_df = pd.DataFrame(label_accuracy)\n",
    "    \n",
    "    return overall_df, label_df\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Load ground truth data\n",
    "    gt_df = pd.read_pickle(\"../data/synthetic/component_series_400.pkl\")\n",
    "    \n",
    "    # Load prediction results\n",
    "    predictions = []\n",
    "    with open(f\"{BASE_DIR}/results/data/synthetic/component_series_400.pkl/gemini-1.5-flash/component.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            predictions.append(json.loads(line.strip()))\n",
    "    \n",
    "    # Process data\n",
    "    y_true, y_pred = process_data(gt_df, predictions)\n",
    "    \n",
    "    # Debug output\n",
    "    print(\"len(y_true), len(y_pred)\", len(y_true), len(y_pred))\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    metrics = evaluate_multi_label(y_true, y_pred)\n",
    "    \n",
    "    # Convert results to DataFrames\n",
    "    overall_metrics_df, label_accuracy_df = create_metrics_dataframe(metrics)\n",
    "    \n",
    "    # Output results\n",
    "    print(f\"Number of data samples: {len(y_true)}\")\n",
    "    \n",
    "    print(\"\\n===== Overall Evaluation Metrics =====\")\n",
    "    print(overall_metrics_df.to_string(index=False, float_format=lambda x: f\"{x*100:.2f}\"))\n",
    "    \n",
    "    print(\"\\n===== Label-specific Accuracy =====\")\n",
    "    print(label_accuracy_df.to_string(index=False, float_format=lambda x: f\"{x*100:.2f}\"))\n",
    "    \n",
    "    # Save DataFrames to CSV (optional)\n",
    "    overall_metrics_df.to_csv(\"overall_metrics.csv\", index=False)\n",
    "    label_accuracy_df.to_csv(\"label_accuracy.csv\", index=False)\n",
    "    \n",
    "    # Return combined results (for use if needed)\n",
    "    combined_results = {\n",
    "        'sample_count': len(y_true),\n",
    "        'overall_metrics': overall_metrics_df,\n",
    "        'label_accuracy': label_accuracy_df\n",
    "    }\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = main()\n",
    "overall_df = results['overall_metrics']\n",
    "label_df = results['label_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(label_df['Label'], label_df['Accuracy'])\n",
    "plt.title('Label-wise Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "#plt.savefig('label_accuracy.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
