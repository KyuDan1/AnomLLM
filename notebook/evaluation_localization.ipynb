{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.chdir('/home/nas4_user/hojuncho/kyudan/AnomLLM/AnomLLM/'))\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import time_series_to_image, LIMIT_PROMPT, PROMPT, time_series_to_str\n",
    "from utils import view_base64_image, display_messages, collect_results\n",
    "from utils import interval_to_vector, vector_to_interval\n",
    "from data.synthetic import SyntheticDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from affiliation.generics import convert_vector_to_events\n",
    "from affiliation.metrics import pr_from_events\n",
    "from utils import compute_metrics\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_anomaly_sequences(text):\n",
    "    pattern = r\"anomaly data\\s*\\d+:\\s*([-\\d.\\s]+)\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    anomaly_sequences = []\n",
    "    for match in matches:\n",
    "        # 매치된 문자열을 공백 기준으로 분리하고 float형으로 변환\n",
    "        numbers = [float(num) for num in match.strip().split()]\n",
    "        anomaly_sequences.append(numbers)\n",
    "    \n",
    "    return anomaly_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력이 dictionary의list임.\n",
    "def average_dict_values(dict_list):\n",
    "    sums = {}\n",
    "    counts = {}\n",
    "\n",
    "    # list iter\n",
    "    for d in dict_list:\n",
    "        # dixt iter\n",
    "        for key, value in d.items():\n",
    "            sums[key] = sums.get(key, 0) + value\n",
    "            counts[key] = counts.get(key, 0) + 1\n",
    "            \n",
    "    # 각 키에 대해 평균 계산: 합계 / 등장횟수\n",
    "    averages = {key: sums[key] / counts[key] for key in sums}\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_name!!\n",
    "data_name_list = [\"freq\", \"point\", \"range\", \"trend\"]\n",
    "\n",
    "# Initialize dictionaries instead of lists\n",
    "anomaly_indexs = {data_name: {} for data_name in data_name_list}\n",
    "normal_indexs = {data_name: {} for data_name in data_name_list}\n",
    "eval_results = {data_name: {} for data_name in data_name_list}\n",
    "\n",
    "for data_name in data_name_list:\n",
    "\n",
    "    # gt 때문에 loading함.\n",
    "    data_dir = f'data/synthetic/{data_name}/eval/'\n",
    "    eval_dataset = SyntheticDataset(data_dir)\n",
    "    eval_dataset.load()\n",
    "\n",
    "    model_name = 'gemini-1.5-flash' #'gpt-4o-mini' #'gemini-1.5-flash'\n",
    "    jsonl_list = glob.glob(f'./results/synthetic/{data_name}/{model_name}/*')\n",
    "\n",
    "    # eval 할 것들.\n",
    "    # jsonl 파일 하나에 대해서 분해를 할 수 있다.\n",
    "\n",
    "    for json_name in jsonl_list:\n",
    "        anomaly_indexs[data_name][json_name] = []\n",
    "        normal_indexs[data_name][json_name] = []\n",
    "        eval_results[data_name][json_name] = []\n",
    "\n",
    "        result_df = pd.read_json(json_name, lines=True)\n",
    "\n",
    "        result_df['input_prompt'] = result_df['request'].apply(lambda x:x['messages'][0]['content'])\n",
    "        result_df['eval_idx'] = result_df['custom_id'].apply(lambda x:int(x.split('_')[-1]))\n",
    "        result_df['incontext'] = result_df['input_prompt'].apply(lambda x:extract_anomaly_sequences(x))\n",
    "\n",
    "        raw_result = collect_results(f'./results/synthetic/{data_name}/{model_name}/')\n",
    "        key_name = json_name.split('/')[-1].replace('.jsonl', '')\n",
    "        target_res = raw_result[f' ({key_name})']\n",
    "\n",
    "        for i, eval_data in enumerate(eval_dataset):\n",
    "            sample_ano_loc, sample_series = eval_data\n",
    "\n",
    "\n",
    "            if sample_ano_loc.shape[-1] == 0:\n",
    "                normal_indexs[data_name][json_name].append(i)\n",
    "                dummy_res = {'precision': 0,\n",
    "                'recall': 0,\n",
    "                'f1': 0,\n",
    "                'affi precision': 0,\n",
    "                'affi recall': 0,\n",
    "                'affi f1': 0}\n",
    "                eval_results[data_name][json_name].append(dummy_res)\n",
    "                continue\n",
    "            \n",
    "            anomaly_indexs[data_name][json_name].append(i)\n",
    "                \n",
    "            res_idx = result_df[result_df.eval_idx == i+1].index[0] # index가 0부터 399까지\n",
    "            #print(res_idx)\n",
    "            pred_vector = target_res[res_idx]\n",
    "            #print(pred_vector)\n",
    "            gt = interval_to_vector(sample_ano_loc[0], end=1000)\n",
    "            #print(gt)\n",
    "            our_metric = compute_metrics(gt, pred_vector)\n",
    "            eval_results[data_name][json_name].append(our_metric)\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                print(i, sample_ano_loc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name_list = [\"freq\", \"point\", \"range\", \"trend\"]\n",
    "\n",
    "model_name = 'gemini-1.5-flash' #'gpt-4o-mini' #'gemini-1.5-flash'\n",
    "\n",
    "for data_name in data_name_list:\n",
    "    \n",
    "    jsonl_list = glob.glob(f'./results/synthetic/{data_name}/{model_name}/*')\n",
    "    for jsonl in jsonl_list:\n",
    "        total_result = average_dict_values(eval_results[data_name][jsonl])\n",
    "        print(f\"{data_name}_{jsonl}\")\n",
    "        for k, v in total_result.items():\n",
    "            print(f\"{k}: {v*100:.2f}\")\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a table with all data (including data without anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "data_name_list = [\"freq\", \"point\", \"range\", \"trend\"]\n",
    "model_name = 'gemini-1.5-flash'  # 'gpt-4o-mini' #'gemini-1.5-flash'\n",
    "\n",
    "# 결과를 저장할 리스트 생성\n",
    "table_data = []\n",
    "\n",
    "# 헤더 정의\n",
    "headers = [\"Data Type\", \"Model Type\", \"Precision\", \"Recall\", \"F1\", \"Affi Precision\", \"Affi Recall\", \"Affi F1\"]\n",
    "\n",
    "# 각 열의 최대 너비 계산을 위한 딕셔너리\n",
    "col_width = {header: len(header) for header in headers}\n",
    "\n",
    "# 데이터 수집\n",
    "for data_name in data_name_list:\n",
    "    jsonl_list = glob.glob(f'./results/synthetic/{data_name}/{model_name}/*')\n",
    "    \n",
    "    for jsonl in jsonl_list:\n",
    "        # 파일 이름만 추출 (경로 제외)\n",
    "        model_type = os.path.basename(jsonl).replace('.jsonl', '')\n",
    "        \n",
    "        # 결과 계산\n",
    "        total_result = average_dict_values(eval_results[data_name][jsonl])\n",
    "        \n",
    "        # 행 데이터 생성\n",
    "        row = [\n",
    "            data_name,\n",
    "            model_type,\n",
    "            f\"{total_result['precision']*100:.2f}\",\n",
    "            f\"{total_result['recall']*100:.2f}\",\n",
    "            f\"{total_result['f1']*100:.2f}\",\n",
    "            f\"{total_result['affi precision']*100:.2f}\",\n",
    "            f\"{total_result['affi recall']*100:.2f}\",\n",
    "            f\"{total_result['affi f1']*100:.2f}\"\n",
    "        ]\n",
    "        \n",
    "        # 열 너비 업데이트\n",
    "        for i, value in enumerate(row):\n",
    "            col_width[headers[i]] = max(col_width[headers[i]], len(value))\n",
    "            \n",
    "        table_data.append(row)\n",
    "\n",
    "# 테이블 출력 형식 정의\n",
    "format_str = \" | \".join([f\"{{:<{col_width[header]}}}\" for header in headers])\n",
    "separator = \"-+-\".join([\"-\" * col_width[header] for header in headers])\n",
    "\n",
    "# 헤더 출력\n",
    "print(format_str.format(*headers))\n",
    "print(separator)\n",
    "\n",
    "# 데이터 행 출력\n",
    "for row in table_data:\n",
    "    print(format_str.format(*row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a table with only data that has anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "data_name_list = [\"freq\", \"point\", \"range\", \"trend\"]\n",
    "model_name = 'gemini-1.5-flash'  # 'gpt-4o-mini' #'gemini-1.5-flash'\n",
    "\n",
    "# 결과를 저장할 리스트 생성\n",
    "table_data = []\n",
    "\n",
    "# 헤더 정의\n",
    "headers = [\"Data Type\", \"Model Type\", \"Precision\", \"Recall\", \"F1\", \"Affi Precision\", \"Affi Recall\", \"Affi F1\"]\n",
    "\n",
    "# 각 열의 최대 너비 계산을 위한 딕셔너리\n",
    "col_width = {header: len(header) for header in headers}\n",
    "\n",
    "# 데이터 수집\n",
    "for data_name in data_name_list:\n",
    "    jsonl_list = glob.glob(f'./results/synthetic/{data_name}/{model_name}/*')\n",
    "    \n",
    "    for jsonl in jsonl_list:\n",
    "        # 파일 이름만 추출 (경로 제외)\n",
    "        model_type = os.path.basename(jsonl).replace('.jsonl', '')\n",
    "        \n",
    "        # 결과 계산\n",
    "        anomaly_results = [eval_results[data_name][jsonl][a_idx] for a_idx in anomaly_indexs[data_name][jsonl]]\n",
    "        total_result = average_dict_values(anomaly_results)\n",
    "        \n",
    "        # 행 데이터 생성\n",
    "        row = [\n",
    "            data_name,\n",
    "            model_type,\n",
    "            f\"{total_result['precision']*100:.2f}\",\n",
    "            f\"{total_result['recall']*100:.2f}\",\n",
    "            f\"{total_result['f1']*100:.2f}\",\n",
    "            f\"{total_result['affi precision']*100:.2f}\",\n",
    "            f\"{total_result['affi recall']*100:.2f}\",\n",
    "            f\"{total_result['affi f1']*100:.2f}\"\n",
    "        ]\n",
    "        \n",
    "        # 열 너비 업데이트\n",
    "        for i, value in enumerate(row):\n",
    "            col_width[headers[i]] = max(col_width[headers[i]], len(value))\n",
    "            \n",
    "        table_data.append(row)\n",
    "\n",
    "# 테이블 출력 형식 정의\n",
    "format_str = \" | \".join([f\"{{:<{col_width[header]}}}\" for header in headers])\n",
    "separator = \"-+-\".join([\"-\" * col_width[header] for header in headers])\n",
    "\n",
    "# 헤더 출력\n",
    "print(format_str.format(*headers))\n",
    "print(separator)\n",
    "\n",
    "# 데이터 행 출력\n",
    "for row in table_data:\n",
    "    print(format_str.format(*row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomllm-yItPHLz9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
